# Server Configuration
PORT=3001
NODE_ENV=development

# Deployment Mode
# - 'online': Uses Hugging Face API (free, no API key needed, for development without Docker)
# - 'local': Uses Ollama (requires Docker, for offline/private deployments)
DEPLOYMENT_MODE=online

# Redis Configuration
REDIS_URL=redis://localhost:6379

# Local LLM Configuration (for local deployment)
OLLAMA_URL=http://localhost:11434
OLLAMA_MODEL=codellama

# Online LLM Configuration (for online deployment)
# Uses free/public APIs - no API keys needed
HUGGINGFACE_API_URL=https://api-inference.huggingface.co/models
HUGGINGFACE_MODEL=bigcode/starcoder

# File Upload Limits
MAX_UPLOAD_SIZE=100mb
MAX_ARCHIVE_SIZE=500mb

# Analysis Configuration
ANALYSIS_TIMEOUT=300000
CACHE_TTL=3600
